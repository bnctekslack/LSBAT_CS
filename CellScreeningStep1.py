import os
from io import BytesIO
from typing import Dict, List

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from openpyxl.drawing.image import Image as XLImage
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score
from sklearn.preprocessing import StandardScaler
from k_means_constrained import KMeansConstrained


COLUMNS_TO_ANALYZE = {
    "All item": [
        "Capacity(Ah)",
        "Weight(g)",
        "Height(mm)",
        "Width(mm)",
        "Initial Voltage(V)",
        "100% Voltage(V)",
        "0% Voltage(V)",
        "50% Voltage(V)",
        "Initial ACIR(mÎ©)",
        "100% ACIR(mÎ©)",
        "0% ACIR(mÎ©)",
        "50% ACIR(mÎ©)",
    ]
}

STD_COLS = [
    "Capacity(Ah)",
    "Weight(g)",
    "Height(mm)",
    "Width(mm)",
    "Initial Voltage(V)",
    "100% Voltage(V)",
    "0% Voltage(V)",
    "50% Voltage(V)",
    "Initial ACIR(mÎ©)",
    "100% ACIR(mÎ©)",
    "0% ACIR(mÎ©)",
    "50% ACIR(mÎ©)",
]

DEFAULT_OUTPUT_DIR = "Results"

# ê¶Œì¥ ê°€ì¤‘ì¹˜ ì„¤ì •
DEFAULT_WEIGHTS = {
    # Tier 1: ì„±ëŠ¥/ì•ˆì „ ì§ê²°
    "Capacity(Ah)": 3.0,
    "Initial Voltage(V)": 3.0,
    "Initial ACIR(mÎ©)": 2.5,
    
    # Tier 2: ìš´ì˜ íŠ¹ì„±
    "100% ACIR(mÎ©)": 2.0,
    "0% ACIR(mÎ©)": 2.0,
    "50% ACIR(mÎ©)": 2.0,
    
    "100% Voltage(V)": 1.0,
    "0% Voltage(V)": 1.0,
    "50% Voltage(V)": 1.0,
    
    # Tier 3: ë¶€ê°€ íŠ¹ì„±
    "Weight(g)": 1.5,
    
    # Tier 4: ë¬¼ë¦¬ì  í˜¸í™˜ì„±
    "Height(mm)": 0.5,
    "Width(mm)": 0.5,
}

class BalancedKMeans:
    def __init__(self, n_clusters=10, max_iter=100, random_state=42, tol=1e-4):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.random_state = np.random.RandomState(random_state)
        self.tol = tol
        self.cluster_centers_ = None
        self.labels_ = None

    def _kmeans_plus_plus_init(self, X):
        n, d = X.shape
        centers = np.empty((self.n_clusters, d), dtype=float)
        idx = self.random_state.randint(0, n)
        centers[0] = X[idx]
        closest_dist_sq = np.sum((X - centers[0]) ** 2, axis=1)
        for c in range(1, self.n_clusters):
            probs = closest_dist_sq / closest_dist_sq.sum()
            idx = self.random_state.choice(n, p=probs)
            centers[c] = X[idx]
            new_dist_sq = np.sum((X - centers[c]) ** 2, axis=1)
            closest_dist_sq = np.minimum(closest_dist_sq, new_dist_sq)
        return centers

    def _balanced_assignment(self, X, centers, caps):
        n = X.shape[0]
        k = centers.shape[0]
        dists = np.linalg.norm(X[:, None, :] - centers[None, :, :], axis=2)
        order = np.argmin(dists, axis=1)
        base = dists[np.arange(n), order]
        idx_order = np.argsort(base)
        labels = -np.ones(n, dtype=int)
        caps_left = caps.copy()
        nearest_rank = np.argsort(dists, axis=1)
        for i in idx_order:
            for j in range(k):
                c = nearest_rank[i, j]
                if caps_left[c] > 0:
                    labels[i] = c
                    caps_left[c] -= 1
                    break
        if np.any(labels == -1):
            for i in np.where(labels == -1)[0]:
                c = np.argmax(caps_left)
                labels[i] = c
                caps_left[c] -= 1
        return labels

    def fit_predict(self, X):
        n = X.shape[0]
        k = self.n_clusters
        base = n // k
        r = n % k
        capacities = np.array([base + 1 if i < r else base for i in range(k)], dtype=int)
        centers = self._kmeans_plus_plus_init(X)
        labels_prev = None
        for _ in range(self.max_iter):
            labels = self._balanced_assignment(X, centers, capacities)
            new_centers = np.array([X[labels == c].mean(axis=0) for c in range(k)])
            shift = np.linalg.norm(new_centers - centers)
            centers = new_centers
            if labels_prev is not None and np.all(labels == labels_prev):
                break
            if shift < self.tol:
                break
            labels_prev = labels
        self.cluster_centers_ = centers
        self.labels_ = labels
        return labels


def _normalize_scores(scores: List[float], mode: str) -> np.ndarray:
    arr = np.array(scores)
    if arr.size == 0:
        return arr
    span = np.max(arr) - np.min(arr)
    if span == 0:
        return np.ones_like(arr)
    if mode == "dbi":
        return 1 - (arr - np.min(arr)) / span
    return (arr - np.min(arr)) / span


def compute_k_metrics(df: pd.DataFrame) -> Dict[str, Dict[str, object]]:
    results = {}
    for name, cols in COLUMNS_TO_ANALYZE.items():
        if any(col not in df.columns for col in cols):
            continue
        data = df[cols].dropna().values
        if data.size == 0:
            continue

        K_range = range(9, 15) ################## K ####################
        dbi_scores, chi_scores = [], []
        for k in K_range:
            kmeans = KMeansConstrained(
                n_clusters=k,
                size_min=int(data.shape[0] / k * 0.8),
                size_max=int(data.shape[0] / k * 1.2),
                random_state=42,                #ì˜ë¯¸ì—†ëŠ” ì‹œì‘ê°’ 0~100
            )
            labels = kmeans.fit_predict(data)
            dbi_scores.append(davies_bouldin_score(data, labels))       #DBI ì ìˆ˜ ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ â†“)
            chi_scores.append(calinski_harabasz_score(data, labels))    #CHI ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ â†‘)

        dbi_norm = _normalize_scores(dbi_scores, "dbi")
        chi_norm = _normalize_scores(chi_scores, "chi")
        combined_score = 0.5 * dbi_norm + 0.5 * chi_norm
        optimal_k_final = K_range[int(np.argmax(combined_score))]
        print(f"[{name}] Optimal K (Combined): {optimal_k_final}")
        results[name] = {
            "k_values": list(K_range),
            "dbi_scores": dbi_scores,
            "chi_scores": chi_scores,
            "optimal_k_dbi": K_range[int(np.argmin(dbi_scores))],
            "optimal_k_chi": K_range[int(np.argmax(chi_scores))],
            "optimal_k_final": optimal_k_final,
        }
    return results


def _add_k_selection_sheet(workbook, k_results: Dict[str, Dict[str, object]]):
    ws = workbook.create_sheet("K_Selection_Result")
    ws.append(["ë¶„ì„ ëŒ€ìƒ", "DBI k", "CHI k", "ìµœì¢… ì¶”ì²œ k"])

    for name, result in k_results.items():
        ws.append(
            [
                name,
                result["optimal_k_dbi"],
                result["optimal_k_chi"],
                result["optimal_k_final"],
            ]
        )
        fig, axs = plt.subplots(1, 2, figsize=(10, 4))
        axs[0].plot(result["k_values"], result["dbi_scores"], "o-", label="DBI (â†“)")
        axs[0].axvline(result["optimal_k_dbi"], color="r", linestyle="--", label="Optimal DBI")
        axs[0].set_title("Davies-Bouldin Index")
        axs[0].set_xlabel("k")
        axs[0].legend()

        axs[1].plot(result["k_values"], result["chi_scores"], "o-", label="CHI (â†‘)")
        axs[1].axvline(result["optimal_k_chi"], color="r", linestyle="--", label="Optimal CHI")
        axs[1].set_title("Calinski-Harabasz Index")
        axs[1].set_xlabel("k")
        axs[1].legend()

        fig.suptitle(f"Optimal K Determination for {name}", fontsize=14)
        plt.tight_layout(rect=[0, 0, 1, 0.95])

        img_data = BytesIO()
        plt.savefig(img_data, format="png", bbox_inches="tight")
        plt.close(fig)
        img = XLImage(img_data)
        img.width, img.height = 600, 250
        ws.add_image(img, f"G{ws.max_row}")


def run_step1(
    cs0_file: str, 
    output_dir: str = DEFAULT_OUTPUT_DIR,
    weights: dict = None,  # ìƒˆ íŒŒë¼ë¯¸í„°
    use_equal_weights: bool = False  # ê· ë“± ê°€ì¤‘ì¹˜ ì˜µì…˜
):
    #     
    df = pd.read_excel(cs0_file, sheet_name="Non_Outliers_List")
    k_results = compute_k_metrics(df)
    if "All item" not in k_results:
        raise ValueError("í•„ìš”í•œ ë¶„ì„ í•­ëª© ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    k = k_results["All item"]["optimal_k_final"]
    id_col = "Lot Number"
    feature_cols = COLUMNS_TO_ANALYZE["All item"]

    df_use = df[[id_col] + feature_cols].dropna().reset_index(drop=True)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_use[feature_cols])

    bkm = BalancedKMeans(n_clusters=k, random_state=42)
    labels = bkm.fit_predict(X_scaled)
    df_use["cluster"] = labels
    df_out = df.merge(df_use[[id_col, "cluster"]], on=id_col, how="left")

    df_scaled = df_use.copy()
    df_scaled[feature_cols] = X_scaled
    df_std = df_scaled.copy()
    df_std["cluster"] = labels

    df_counts = df_use["cluster"].value_counts().sort_index().reset_index()
    df_counts.columns = ["cluster", "count"]

    # í‘œì¤€í¸ì°¨ ê³„ì‚°
    std_cols_use = [c for c in STD_COLS if c in df_out.columns]
    df_cluster_std = df_out.groupby("cluster")[std_cols_use].std().reset_index()
    
    # ìˆœìœ„ ë³€í™˜
    df_rank = df_cluster_std.copy()
    for col in std_cols_use:
        df_rank[col] = df_cluster_std[col].rank(method="min", ascending=True)
    
    # ========== ê°€ì¤‘ì¹˜ ì ìš© ë¶€ë¶„ ==========
    if use_equal_weights:
        # ê· ë“± ê°€ì¤‘ì¹˜ (ê¸°ì¡´ ë°©ì‹)
        print("âš–ï¸  ê· ë“± ê°€ì¤‘ì¹˜ ì‚¬ìš© (ëª¨ë“  í•­ëª© 1.0)")
        weights_to_use = {col: 1.0 for col in std_cols_use}
    else:
        # ê°€ì¤‘ì¹˜ ì ìš©
        if weights is None:
            weights_to_use = DEFAULT_WEIGHTS
            print("ğŸ¯ ê¶Œì¥ ê°€ì¤‘ì¹˜ ì ìš©:")
        else:
            weights_to_use = weights
            print("ğŸ¯ ì‚¬ìš©ì ì •ì˜ ê°€ì¤‘ì¹˜ ì ìš©:")
        
        # ê°€ì¤‘ì¹˜ ì¶œë ¥
        for col in std_cols_use:
            w = weights_to_use.get(col, 1.0)
            print(f"   {col:25s}: {w:.1f}")
    
    # ê°€ì¤‘ì¹˜ ì ìš© ìˆœìœ„ í•©ì‚°
    df_rank["total_rank"] = sum(
        df_rank[col] * weights_to_use.get(col, 1.0)
        for col in std_cols_use
    )
    # =====================================
    
    # ìµœê³  / ìµœì•… í´ëŸ¬ìŠ¤í„° ì„ ì •
    best_cluster = int(df_rank.loc[df_rank["total_rank"].idxmin(), "cluster"]) if len(df_rank) else None
    worst_cluster = int(df_rank.loc[df_rank["total_rank"].idxmax(), "cluster"]) if len(df_rank) else None
    print(f"ğŸŒŸ ê°€ì¥ ì•ˆì •ì ì¸ í´ëŸ¬ìŠ¤í„°: {best_cluster}")
    print(f"âš ï¸ ê°€ì¥ ë³€ë™ì„±ì´ í° í´ëŸ¬ìŠ¤í„°: {worst_cluster}")
    
    # ê°€ì¤‘ì¹˜ ì •ë³´ ì¶”ê°€ ì €ì¥
    df_weights = pd.DataFrame([
        {"í•­ëª©": col, "ê°€ì¤‘ì¹˜": weights_to_use.get(col, 1.0)}
        for col in std_cols_use
    ])
    
    os.makedirs(output_dir, exist_ok=True)
    base_path = os.path.join(output_dir, "Step1_Results.xlsx")
    cs1_path = base_path
    if os.path.exists(base_path):
        try:
            os.remove(base_path)
        except PermissionError:
            ts = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
            cs1_path = os.path.join(output_dir, f"Step1_Results_{ts}.xlsx")
            print(f"[WARN] {base_path} íŒŒì¼ì„ ë®ì–´ì“¸ ìˆ˜ ì—†ì–´ ìƒˆ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤: {cs1_path}")

    with pd.ExcelWriter(cs1_path, engine="openpyxl") as writer:
        df_out.to_excel(writer, sheet_name="Original_Data", index=False)
        df_std.to_excel(writer, sheet_name="Clustered_StandardScaler", index=False)
        df_counts.to_excel(writer, sheet_name="Cluster_Counts", index=False)
        
        for c in sorted(df_out["cluster"].dropna().unique()):
            cluster_members = df_out[df_out["cluster"] == c][["Lot Number"] + feature_cols]
            cluster_members.to_excel(writer, sheet_name=f"Cluster{int(c)}", index=False)
        
        df_cluster_std.to_excel(writer, sheet_name="Cluster_STD", index=False)
        df_rank.to_excel(writer, sheet_name="Cluster_STD_Rank", index=False)
        df_weights.to_excel(writer, sheet_name="Applied_Weights", index=False)  # ìƒˆ ì‹œíŠ¸
        _add_k_selection_sheet(writer.book, k_results)
    
    print("âœ… ì €ì¥ ì™„ë£Œ:", cs1_path)
    return cs1_path, best_cluster, worst_cluster


__all__ = ["run_step1"]
